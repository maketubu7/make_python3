machine learning

1、数据
    data set 数据集
    sample 样本
    feature 特征
    label 标记
    feature space 特征空间
    图像 每一个像素点都是特征
2、机器学习的基本任务（监督学习）
    1、分类任务
        二分类
            相近物体识别
        多分类
            数字识别 图像识别 信用卡风险判别
        多标签分类
            图像内容识别分类
        一些算法只支持二分类任务，但是多分类任务可以转换为二分类任务
        有一些算法天然可以完成多分类任务
    2、回归任务
        结果是一个连续数字的值，而非一个类别
        一些算法只能解决回归问题，一些只能解决分类问题，一些二者都能解决
        在一些情况下，回归任务可以简化为分类任务
3、机器学习的分类
    1、监督学习
        给机器学习的训练数据拥有标记或者答案
    2、非监督学习
        定义 - 给机器学习的训练数据没有任何标记或者答案
        对没有标记的数据进行分类 - 聚类分析
        对数据进行降维处理(意义：方便可视化)
            特征提取
            特征压缩： PCA
        意义-异常检测
    3、半监督学习
        定义 - 一部分数据有标记或者答案，另一部分没有
        通常使用无监督学习手段对数据做处理，之后使用监督学习处理
    4、增强学习
        定义 - 根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式

4、机器学习的其他分类
    1、批量学习 batch learning
    2、离线学习 offline learning
    3、在线学习 online learning

    4、参数学习
        一旦学习到了参数，就不在需要原有的数据集（线性回归）
    5、非参数学习
        不对模型进行过多假设，但是非参数不等于没参数

    奥卡姆的剃刀-简单的就是好的
    没有免费的午餐定理
    可以严格的数学推导出 任意两个算法
    他们的期望性能是相同的

5、numpy的简单使用
    1、创建数据
    np.zeros(10,dtype=int)  长度为10的，值为0的向量
    np.zeros(shape=(3,5),dtype=int)  3行5列的全是0的矩阵
    np.ones()同理

    np.full(fill_value=666,shape=(3,5)) 3行5列的全是666的矩阵

    np.arange(0,20,2) 左闭右开步长为2的向量
    np.arange(10) 1-10的向量

    np.linspace(0,20,10) 左闭右闭的区间，切出10个间距相等的数 array([ 0.        ,  2.22222222,  4.44444444,  6.66666667,  8.88888889,
       11.11111111, 13.33333333, 15.55555556, 17.77777778, 20.        ])

    np.linspace(0,20,11) array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20.])

    np.random.randint(0,10) 10 左闭右开 随机取0-10的随机数
    np.random.randint(4,8,size=10) 左闭右开 随机取4-8的随机数 取10个
    np.random.randint(4,8,size=(3,5)) 左闭右开 随机取4-8的随机数 取3行5列的矩阵

    np.random.seed(666)  设置随机显示设置，则随机的概率一直
    np.random.random()  生成0-1的随机浮点数
    np.random.random((3,5)) 生成0-1的随机浮点数 取3行5列的矩阵

    np.random.normal(10,100) 生成均值为10 方差为100的数
    np.random.normal(0,1，size=(3,5)) 生成均值为0 方差为1的数的3行5列的矩阵

    2、array的基本操作

    x = np.arange(10).reshape(2,5)  重分为2行5列的矩阵
    x.ndim  返回为x的维度
    x.shape  返回x的行列
    x.size  返回x的大小

    x[0] 返回向量中索引0的元素
    X[0,1] 返回矩阵的 0行1列的元素
    x[0:5] 向量的切片 返回左闭右开的索引对应的元素
    x[::2] 返回步长为2的向量
    x[::-1] 返回倒序的向量

    X[:2,:3] 返回数组的前2行的前3列
    X[:2,::2] 前2行，间隔为2的去取列
    X[::-1,::-1] 全部倒序重新排列
    sub_x = X[:2,:3].copy()
    全矩阵的子矩阵若被修改，则全矩阵也会被修改，若想重新复制，则调用copy方法
    X.reshape(10,-1) 重新分为10行，多少列自己计算，但是必须被10整除

    3、合并操作
    x = np.array([1,2,3])
    y = np.array([1,2,3])
    np.concatenate([x,y]) axis为控制方向

    np.vstack([x,y]) 垂直方向上的合并
    np.hstack([x,y]) 在水平方向上的合并

    4、分割操作
    np.split(x,[3,7])  3,7为分割点
    np.split(X,[2],axis=1) 按列进行分割

    np.vspilt(X,[2])  垂直分割
    np.hspilt(X,[2])  水平分割

6、array的运算
    2 * x 为其中每个元素乘以2
    x / 2 结果为浮点数
    x // 2 结果为整数
    np.abs(x) 取绝对值
    np.cos(x) 取余弦函数
    np.exp(x)  e的幂次方
    np.sqrt(x) 开方
    np.power(3,x) 三次方
    np.log2(x) 以2为底的对数
    np.log10(x) 以10为底的对数

    A + B 对应元素相加
    A * B 每个元素相乘
    A / B 每个元素相除
    A.dot(B) 矩阵的乘数
    A.T 举证的转置

    np.tile(v,(2,2)) 在行上堆叠两次，列上堆叠两次

    np.linalg.inv(A) 求矩阵的逆（方阵）
    np.linalg.pinv(A) 求矩阵的伪逆矩阵

7、聚合运算
    np.sum(x) 求x的和
    np.min(x) 求最小值
    np.max(x) 求最大值

    np.sum(X,axis=1) 求每行的和，
    np.sum(X,axis=0) 求每列的和，
    np.prod(x) x中的每个元素相乘
    np.mean(x) x的平均值
    np.median(x) x的中位数

    np.percentile(b,q=50) 百分位数 q为百分之多少，百分之多少小于这个数 100为最大值，0为最小值，50为中位数

    np.var(x) 方差
    np.std(x) 标准差

8、排序索引的相关运算
    np.argmin(x)  返回最小值索引
    np.argmax(x)  返回最大值索引

    np.sort(X,axis=0) 按列排序 沿着行排序
    np.sort(X,axis=1)  按行排序 沿着列排序

    np.argsort(x) 排序，返回排序后的索引
    np.partition(x,4) 返回在索引位置为4的地方，前面的比他小，后面的比他大，那么索引4的位置就是第4大的位置
    np.argpartition(x,4) 同上返回相应的索引值
    np.partition(X,2,axis=1) 按行找到第二大的数
    np.partition(X,2,axis=0) 按列找到第二大的数


模型基础
1、参数
    超参数：在算法运行前需要决定的参数，调参就是超参数
        寻找好的超参数：领域知识，经验数值，试验搜索
        在参数列表中计算最高准确度的参数（在参数列表中找到参数后，可以适当拓宽参数的范围）
    模型参数：算法过程中学习的参数

2、KNN算法
    1、n_neigbers 计算距离的个数
    2、距离权重考量 distance
    3、距离参数 p 明可夫斯基距离的参数p p=1 为曼哈顿距离， p=2为欧拉距离
    4、其他超参数 http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html

3、网格搜索确定最好超参数
    1、网格搜素参数 knn_clf 空参数KNN， param_grid参数列表， n_jobs 为工作的核数， verbose 打印搜索过程
    grid_search = GridSearchCV(knn_clf, param_grid, n_jobs=-1, verbose=2)

4、数据归一化（各个指标间的相互影响，有些尺度打，有的尺度小）
    1、解决方案：把所有数据映射到统一尺度
        1、最值归一化 normalization：把所有数据映射到0-1之间 x_scale = (x-x_min) / (x_max-x_min)
            适用于分布有明显边界的情况；受outlier影响比较大
        2、均值方差归一化 standardization:把所有的数据归一到均值为0方差为1的分布中
            适用于分布没有明显的边界：有可能存在极端数据值 x_scale = x-x_mean / std
    2、测试数据如何归一化
        测试数据是模拟真实环境
            真实环境很有可能无法得到所有测试的数据的均值和方差
            对数据的归一化也是算话的一部分
            (x_test - mean_train) / std_train
        sklearn中的Scalar from sklearn.preprocessing import StandardScaler
        工具处理归一化
            standardScalar.fit(X_train)  理解为训练数据集
            X_trains = tandardScalar.transform(X_train)  转化数据集为归一化数据集
            X_test_standard = standardScalar.transform(X_test)  对测试数据集归一化
            ！！！注意，此时不能传入没有归一化的数据！ 不然结果非常的差
            然后调用相关算法对数据集进行训练和测试

5、线性回归算法
    1、特征
        1、解决回归问题
        2、许多强大的非线性算法的基础
        3、具有强大的柯解释性
        4、典型的参数学习
        5、只能解决回归问题
        6、对数据有假设： 存在线性关系 （对比KNN 对数据没有假设）
    2、我们希望y^i 与 y_i 的差距尽量小，那么表达y^i 和 y_i的差距 （y_i 真值 y^i 预测值）
        (y_i-y^i)^2 用差值的平方来表示
        那么我们的目标就是他们的和尽可能的小, 及(y_i - ax_i -b)^2尽可能的小
        损失函数（loss function）, 效用函数（utility function）
        通过分析问题，确定问题的损失函数或者效用函数，通过最优化损失函数或者效用函数，获得机器学习的模型
        近乎所有的参数学习算法都是这样的算法，比如
        线性回归 svm 多项式回归 神经网络 逻辑回归 ----> 最优化原理

    3、最小二乘法求解 最佳参数
        找到a和b 使得(y_i - ax_i -b)^2的和尽可能的小 fit过程求解a与b的值
        self.a_ = (x_train - x_mean).dot(y_train - y_mean) / (x_train - x_mean).dot(x_train - x_mean)
        self.b_ = y_mean - self.a_ * x_mean

    4、回归算法的评价
        衡量标准（y_test_i - y_test^i）^2 求和
        问题和求和的m相关，于是我们要求 均方误差 （mean squared error）（y_test_i - y_test^i）^2 / m
        问题 量纲 于是我们要求 均方根误差 对均方误差开根号（mean squared error）（y_test_i - y_test^i）^2 / m 开根号
        平均绝对误差 |y_test_i - y_test^i| 求和 除以 m (mean absolute error)
        from sklearn.metrics import mean_squared_error 均方误差
        from sklearn.metrics import mean_absolute_error 平均绝对误差

        被sklearn引用的误差评价为 R^2 = 1-（ss_residual/ss_total）==> 1-SUM(y^i-y_i)^2/SUM(y_mean-y_i)^2
        ss_residual 模型预测产生的错误
        ss_total 使用Baseline mode 预测产生的误差

        R^2 越大越好，当我们的预测模型不犯任何错误，R^2为最大值1
        当我们的模型等于基准模型时，R^2 = 0
        如果R^2 < 0 说明学习模型还不如基准模型，此时说明我们的数据不存在任何线性关系
    5、优点
        对数据具有强解释性，能看出各个特征的相关性

6、多元线性回归(不需要数据归一化)
    x^i = (x1^i,x2^i,x3^i,x4^i,x5^i,x6^i,....)
    y = a1x1 + a2x2 + a3x3 + ... + anxn + b  求解系数a 与 b
    a = (.....) a0 截距 a1-an 系数
    a = (x_b^Tx_b)^-1x_b^Ty x_b 即为加了一个参数1 的特征向量集合
    from sklearn.linear_model import LinearRegression

7、梯度下降法
    1、特征
        不是一个机器学习算法
         是一种基于搜索的最优化方法
         作用 最小化已给损失函数
         梯度上升法：最大化一个效用函数
    2、寻找使得损失函数为最小值的参数系数
        e 为学习率（learning rate）
        e 的取值影响最优解的获取速度 太小减慢收敛的速度， 太大可能无法收敛
        并不是所有的函数都有极值点
        解决方案 多次运行，随机化初始化点， 梯度下降法的初始点也是一个超参数
    3、核心理解代码
        epsilon = 1e-8
        eta = 0.1   # 学习率
        def J(theta):  #损失函数
            return (theta-2.5)**2 - 1.

        def dJ(theta):   #原函数的导数
            return 2*(theta-2.5)

        theta = 0.0
        while True:
            gradient = dJ(theta)
            last_theta = theta
            theta = theta - eta * gradient

            if(abs(J(theta) - J(last_theta)) < epsilon):
                break
    4、多元线性回归梯度下降法
        def fit_gd(self, X_train, y_train, eta=0.01, n_iters=1e4):
        """根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型"""
            assert X_train.shape[0] == y_train.shape[0], \
            "the size of X_train must be equal to the size of y_train"

            def J(theta, X_b, y):
                try:
                    return np.sum((y - X_b.dot(theta)) ** 2) / len(y)
                except:
                    return float('inf')

            def dJ(theta, X_b, y):
                res = np.empty(len(theta))
                res[0] = np.sum(X_b.dot(theta) - y)
                for i in range(1, len(theta)):
                    res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
                return res * 2 / len(X_b)
                return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(y)  # 向量法

            def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):
                theta = initial_theta
                cur_iter = 0
                while cur_iter < n_iters:
                    gradient = dJ(theta, X_b, y)
                    last_theta = theta
                    theta = theta - eta * gradient
                    if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
                        break
                    cur_iter += 1
                return theta

            X_b = np.hstack([np.ones((len(X_train), 1)), X_train])
            initial_theta = np.zeros(X_b.shape[1])
            self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)

8、随机梯度下降法
    1、特征
        1、拥有不确定性
        2、用空间换时间
        3、学习率需要逐渐递减的，保证不会跳出极值点 模拟退火的思想 学习率如下 e = t0 / i_iters + t1













